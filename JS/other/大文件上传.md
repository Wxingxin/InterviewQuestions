
### 一、为什么普通 upload 不行？（30 秒开场）

| 文件大小       | 普通 input[type=file] + FormData 上传会发生什么？                                 |
|----------------|-----------------------------------------------------------------------------------|
| < 100MB        | 一般能成                                                                         |
| 500MB~2GB      | 经常卡在 99%，网络抖动一下就全完蛋，重传从头开始，用户想骂人                     |
| > 5GB          | 基本不可能成功（超时、内存爆掉、nginx 502、浏览器直接崩溃）                     |

所以必须把一个“大文件”变成“很多小文件”来传 → 这就是分片上传的核心思想。

### 二、完整流程图（面试必画！画完你就已经赢了一半）

```
用户选文件（3.2GB）
       ↓
前端：计算整个文件的唯一 hash（比如：a1b2c3d4） ← 用来实现秒传 + 断点续传
       ↓
前端请求：GET /upload/check?hash=a1b2c3d4
       ↓
后端返回：{ exist: false, uploadedList: [] }          ← 还没人传过
       ↓
前端：把文件切成很多 5MB 的小块（chunk）
       chunk0, chunk1, chunk2 ... chunk639
       ↓
前端并发（比如一次只发 4 个）上传还没传过的 chunk
       POST /upload/chunk
       body: FormData
             file: Blob(5MB)
             hash: "a1b2c3d4"           ← 整个大文件的 hash
             index: 0                    ← 是第几块
       ↓
后端把每个 chunk 存成临时文件：/tmp/upload/a1b2c3d4-0
       ↓
前端所有分片都传完（或发现本来就全传完了）
       ↓
前端发合并请求：POST /upload/merge
       { hash: "a1b2c3d4", total: 640, filename: "电影.mp4" }
       ↓
后端按 0~639 顺序把所有小文件拼接成一个大文件
       → /www/videos/电影.mp4
       ↓
返回成功，前端提示“上传完成”
```

### 三、前端最核心的 5 个函数（面试让你写，你就写这 5 个）

```ts
// 1. 计算整个文件的 hash（用 SparkMD5，最重要！）
async function calculateHash(file: File): Promise<string> {
  return new Promise((resolve) => {
    const spark = new SparkMD5.ArrayBuffer();
    const reader = new FileReader();
    const chunkSize = 10 * 1024 * 1024;  // 每次读 10MB
    let offset = 0;

    function loadNext() {
      const slice = file.slice(offset, offset + chunkSize);
      reader.readAsArrayBuffer(slice);
    }

    reader.onload = (e) => {
      spark.append(e.target!.result as ArrayBuffer);
      offset += chunkSize;

      if (offset < file.size) {
        loadNext();
      } else {
        resolve(spark.end());   // 最终得到类似 "f3d2e1c8..." 的字符串
      }
    };

    loadNext();
  });
}

// 2. 切片
function createChunks(file: File, chunkSize = 5 * 1024 * 1024) {
  const chunks = [];
  for (let i = 0; i < file.size; i += chunkSize) {
    chunks.push({
      index: chunks.length,
      blob: file.slice(i, i + chunkSize),
    });
  }
  return chunks;
}

// 3. 检查哪些分片已经上传过（支持断点续传和秒传）
async function checkFileExists(hash: string) {
  const resp = await fetch(`/api/upload/check?hash=${hash}`);
  const data = await resp.json();
  return data; // { exist: true/false, uploadedList: [0,1,5,8] }
}

// 4. 上传单个分片（带重试）
async function uploadChunk(chunkData: any, hash: string, retry = 3) {
  const form = new FormData();
  form.append("chunk", chunkData.blob);
  form.append("index", chunkData.index);
  form.append("hash", hash);

  for (let i = 0; i < retry; i++) {
    try {
      const resp = await fetch("/api/upload/chunk", {
        method: "POST",
        body: form,
      });
      if (resp.ok) return true;
    } catch (e) {
      if (i === retry - 1) throw e;
      await new Promise(r => setTimeout(r, 1000 * (i + 1))); // 指数退避
    }
  }
}

// 5. 主函数：控制并发上传（最难，但也是最亮眼的部分）
async function uploadBigFile(file: File) {
  const hash = await calculateHash(file);

  // 1. 秒传检查
  const { exist, uploadedList = [] } = await checkFileExists(hash);
  if (exist) {
    alert("秒传成功！");
    return;
  }

  const chunks = createChunks(file);
  const pool: Promise<any>[] = [];      // 并发池
  const maxConcurrent = 4;              // 同时最多上传 4 个

  for (const chunk of chunks) {
    if (uploadedList.includes(chunk.index)) continue; // 跳过已上传的

    const task = uploadChunk(chunk, hash).then(() => {
      // 任务结束，从池子里移除
      pool.splice(pool.indexOf(task), 1);
    });

    pool.push(task);

    if (pool.length === maxConcurrent) {
      // 等任意一个完成，再继续添加新的
      await Promise.race(pool);
    }
  }

  // 最后等所有都完成
  await Promise.all(pool);

  // 6. 通知后端合并
  await fetch("/api/upload/merge", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      hash,
      total: chunks.length,
      filename: file.name,
    }),
  });

  alert("上传完成！");
}
```

### 四、后端要配合的 3 个接口（你也要能说出来）

```ts
// 1. 检查文件是否已存在 + 已上传了哪些分片
GET  /api/upload/check?hash=abc123
→ { exist: false, uploadedList: [0,1,3] }

// 2. 上传分片
POST /api/upload/chunk
FormData: chunk, index, hash
→ 后端保存为 ./upload_tmp/abc123-0

// 3. 合并
POST /api/upload/merge
{ hash, total, filename }
→ 后端按顺序把 0~total-1 的临时文件拼成一个完整文件
```

### 五、面试官最爱追问的 8 个点（直接抄答案）

1. 怎么实现秒传？  
   → 先把文件 hash 传给后端，如果数据库里已经有这个 hash，直接返回成功。

2. 怎么实现断点续传？  
   → 刷新页面后再次上传同一个文件，check 接口会返回已上传的 index 列表，前端跳过这些。

3. hash 怎么算得又快又准？  
   → 实际生产可以用“采样hash”：只取文件头 2MB + 尾 2MB + 每隔 2MB 取一点，再 MD5，速度提升 10 倍，碰撞率极低。

4. 并发开多少合适？  
   → 4~6 个最佳，再多会被浏览器/服务器限流，反而慢。

5. 用户中途关页面怎么办？  
   → 已上传的分片还在服务器临时目录，下次打开页面继续传就行（靠 hash 识别）。

6. 服务器怎么合并大文件？  
   → 一定不能把所有分片读到内存！要用文件流顺序追加（Node fs.createWriteStream + appendFile）。

7. 怎么清理没传完的垃圾分片？  
   → 定时任务（每天凌晨）扫描临时目录，超过 24 小时没合并的就删掉。

8. 有没有更简单的方案？  
   → 有！直接用云厂商的“分片上传”：
   - 阿里云 OSS multipart upload
   - 腾讯云 COS 分片上传
   - AWS S3 Multipart Upload
   他们都原生支持，代码只有 20 行，但面试官还是想听你自己手写实现原理。

好了，这次真的是把每个细节都掰开揉碎讲了。你只要把上面这套流程 + 代码背熟，任何公司问“大文件分片上传”，都能稳稳拿下 95+ 分！需要我再给你出一套手写代码题练练吗？

# 

先直接给你一个**完整、能跑的前端 JS 思路 + 代码**，然后边讲边穿插关键点，面试的时候你就可以顺着这个思路说。

> 说明：下面代码默认你有一个后端接口，比如：
>
> * `POST /upload/chunk` 接收单个分片
> * `POST /upload/merge` 让后端把所有分片合并成一个大文件

你在面试里可以只说「后端负责把相同 `fileHash` 的分片按 `index` 排序后写入同一个最终文件」。

---

## 一、分片上传核心思路（面试可以这样讲）

1. **为什么要分片上传？**

   * 文件太大（几百 MB 或几个 GB）时，**一次性上传：**

     * 请求时间长、容易超时
     * 中途失败要从头重来
   * 改成**分片上传**：

     * 把大文件切成很多小块（比如每块 2MB）
     * 一块块传，可以并发
     * 某块失败只重传那一块
     * 甚至可以支持**断点续传**

2. **设计要素：**

   * 分片大小：比如 `2MB` 或 `5MB`
   * 分片编号：`index = 0,1,2,...`
   * 文件唯一标识：`fileHash`（可用文件名 + 大小 + 最后修改时间简化一下）
   * 后端接口：

     * `/upload/chunk`：参数包括 `fileHash`, `chunkIndex`, `totalChunks`
     * `/upload/merge`：参数包括 `fileHash`, `fileName`, `totalChunks`

3. **大致流程：**

   1. 用户选中一个大文件
   2. 前端用 `file.slice` 把文件分片
   3. 依次 / 并发把每个分片 `POST` 给后端
   4. 所有分片传完后，调用 `/upload/merge` 通知后端合并文件
   5. 后端把所有分片合并成最终文件

---

## 二、前端核心代码（原生 JS + fetch）

假设有一个简单的 HTML：

```html
<input type="file" id="fileInput" />
<button id="uploadBtn">上传</button>
<div id="progress"></div>
```

### 1. 核心逻辑 JS

```js
// 分片大小：2MB
const CHUNK_SIZE = 2 * 1024 * 1024;

// 简单生成一个 fileHash（真实项目可以用 spark-md5 做内容 MD5）
function getFileHash(file) {
  return `${file.name}-${file.size}-${file.lastModified}`;
}

// 把文件切成分片
function createFileChunks(file, chunkSize = CHUNK_SIZE) {
  const chunks = [];
  let cur = 0;

  while (cur < file.size) {
    const blob = file.slice(cur, cur + chunkSize);
    chunks.push(blob);
    cur += chunkSize;
  }

  return chunks;
}

// 上传单个分片
async function uploadChunk({ fileHash, chunk, index, total }) {
  const formData = new FormData();
  formData.append('fileHash', fileHash);      // 文件唯一标识
  formData.append('chunkIndex', index);      // 当前分片序号
  formData.append('totalChunks', total);     // 总分片数
  formData.append('chunk', chunk);           // 分片二进制内容

  const res = await fetch('/upload/chunk', {
    method: 'POST',
    body: formData
  });

  if (!res.ok) {
    throw new Error(`chunk ${index} upload failed`);
  }
}

// 通知后端合并
async function mergeRequest({ fileHash, fileName, totalChunks }) {
  const res = await fetch('/upload/merge', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({ fileHash, fileName, totalChunks })
  });

  if (!res.ok) {
    throw new Error('merge failed');
  }

  return res.json();
}

// 并发上传所有分片 + 进度展示
async function uploadFileInChunks(file) {
  const fileHash = getFileHash(file);
  const chunks = createFileChunks(file, CHUNK_SIZE);
  const totalChunks = chunks.length;
  const progressEl = document.getElementById('progress');

  let finished = 0;

  // 控制并发数（比如 3 个一起传）
  const concurrency = 3;
  let current = 0;

  async function run() {
    while (current < totalChunks) {
      const index = current++;
      const chunk = chunks[index];

      try {
        await uploadChunk({ fileHash, chunk, index, total: totalChunks });
        finished++;
        progressEl.innerText = `上传进度：${((finished / totalChunks) * 100).toFixed(2)}%`;
      } catch (e) {
        console.error('chunk upload error', e);
        throw e;
      }
    }
  }

  // 创建 concurrency 个“工人”并发执行
  const workers = [];
  for (let i = 0; i < concurrency; i++) {
    workers.push(run());
  }

  // 等所有分片上传完
  await Promise.all(workers);

  // 通知后端合并
  const result = await mergeRequest({
    fileHash,
    fileName: file.name,
    totalChunks
  });

  progressEl.innerText = '上传完成（已合并）';
  console.log('merge result:', result);
}

// 绑定事件
document.getElementById('uploadBtn').addEventListener('click', async () => {
  const fileInput = document.getElementById('fileInput');
  const file = fileInput.files[0];

  if (!file) {
    alert('请先选择文件');
    return;
  }

  try {
    await uploadFileInChunks(file);
  } catch (e) {
    alert('上传失败：' + e.message);
  }
});
```

---

## 三、面试时可以怎么组织语言（按照这个结构说）

你可以按下面顺序说（边说边写伪代码/关键函数）：

1. **先说明场景 & 问题：**

   * 大文件一次性上传的问题（超时、失败重传成本高）
   * 所以采用**分片上传 + 后端合并**。

2. **描述整体流程：**

   ```text
   1. 前端获取 File 对象
   2. 按固定大小用 file.slice 分片
   3. 每个分片带上 fileHash、chunkIndex、totalChunks 发给 /upload/chunk
   4. 全部分片 OK 后，调用 /upload/merge 通知后端合并
   5. 后端根据 fileHash 找到所有分片，按 index 顺序写入一个文件
   ```

3. **细节可以展开：**

   * 如何生成 `fileHash`（简单版用 文件名+大小+lastModified，复杂版用 MD5）
   * 如何做进度条：当前完成分片数 / 总分片数
   * 如何控制并发数（比如写一个简单的并发任务调度）

4. **可选加分项（有时间就说）：**

   * **断点续传**：

     * 前端先问后端「这个 `fileHash` 已经有哪些 `chunkIndex` 收到了？」
     * 然后只上传缺失的那部分
   * **秒传**：

     * 如果后端发现这个 `fileHash` 对应的完整文件已经存在，可以直接返回「上传完成」，前端就不需要真正传数据。

---

## 四、如果你还想要 Node.js 简单后端示例（可选）

> 面试一般不需要你写完整后端，但你可以口述大概如何实现。

例如（非常简化，只看思路）：

```js
// 假设使用 Express + Node.js，伪代码示意
const express = require('express');
const multer = require('multer');
const fs = require('fs');
const path = require('path');

const upload = multer({ dest: 'temp/' });
const app = express();
app.use(express.json());

app.post('/upload/chunk', upload.single('chunk'), (req, res) => {
  const { fileHash, chunkIndex, totalChunks } = req.body;
  const chunkPath = path.resolve(__dirname, 'chunks', fileHash);

  if (!fs.existsSync(chunkPath)) {
    fs.mkdirSync(chunkPath, { recursive: true });
  }

  // 把临时文件移动到目标分片路径
  const dest = path.resolve(chunkPath, chunkIndex);
  fs.renameSync(req.file.path, dest);

  res.json({ success: true });
});

app.post('/upload/merge', async (req, res) => {
  const { fileHash, fileName, totalChunks } = req.body;
  const chunkPath = path.resolve(__dirname, 'chunks', fileHash);
  const finalPath = path.resolve(__dirname, 'uploads', fileName);

  const writeStream = fs.createWriteStream(finalPath);

  for (let i = 0; i < totalChunks; i++) {
    const chunkFilePath = path.resolve(chunkPath, String(i));
    const data = fs.readFileSync(chunkFilePath);
    writeStream.write(data);
  }

  writeStream.end();
  // TODO: 删除分片目录

  res.json({ success: true, url: `/uploads/${fileName}` });
});

app.listen(3000, () => console.log('Server started on 3000'));
```

---

如果你愿意，我也可以帮你**浓缩成一个“背答案版本”**，比如把上面的内容压缩成一段 1～2 分钟可说完的回答，适合你面试时直接输出。
